"""
    This is a basic framework for depleting human and other contaminant 
    reads from NGS data.  All non-human reads should remain behind.
	
	note: the length of runtime per step is highly variable depending on
	the input data.  It's probably worth trying to consolidate some steps
	in python anyway, given that we're already past the 4hr mark and are
	asking for 10GB RAM for many steps.  Eventually replace with something
	TANGO-based.
"""

__author__ = 'Kristian Andersen <andersen@broadinstitute.org>, Daniel Park <dpark@broadinstitute.org>'

from snakemake.utils import makedirs
import os

rule all_deplete:
    input:
        expand("{dataDir}/{subdir}/{sample}.{adjective}.bam",
            dataDir=config["dataDir"],
            subdir=config["subdirs"]["per_sample"],
            adjective=['raw','cleaned','taxfilt'],
            sample=read_samples_file(config["samples_depletion"])),
    params: LSF="-N"


"""
rule revert_bam:
    input:  config["dataDir"]+'/'+config["subdirs"]["source"]+'/{sample}.bam'
    output: config["dataDir"]+'/'+config["subdirs"]["depletion"]+'/{sample}.raw.bam'
    resources: mem=3
    params: LSF='-W 4:00 -sp 60',
            logid="{sample}"
    run:
            makedirs(expand("{dir}/{subdir}",
                dir=[config["dataDir"],config["tmpDir"]],
                subdir=config["subdirs"]["depletion"]))
            shell("{config[binDir]}/read_utils.py revert_bam_picard {input} {output} --picardOptions SORT_ORDER=queryname SANITIZE=true")

rule deplete_bmtagger:
    ''' This step depletes human reads using BMTagger
        This sometimes takes just over 4 hrs for highly dense lanes
    '''
    input:  config["dataDir"]+'/'+config["subdirs"]["depletion"]+'/{sample}.raw.bam'
    #       expand("{dbdir}/{db}.{suffix}",
    #           dbdir=config["bmTaggerDbDir"],
    #           db=config["bmTaggerDbs_remove"],
    #           suffix=["bitmask","srprism.idx","srprism.map"])
    output: config["tmpDir"]+ '/'+config["subdirs"]["depletion"]+'/{sample}.bmtagger_depleted.bam'
    resources: mem=10
    params: LSF='-q forest',
            refDbs = expand("{dbdir}/{db}", dbdir=config["bmTaggerDbDir"], db=config["bmTaggerDbs_remove"]),
            logid="{sample}"
    shell:  "{config[binDir]}/taxon_filter.py deplete_bam_bmtagger {input} {params.refDbs} {output}"

rule rmdup_mvicuna:
    ''' This step removes PCR duplicate reads using M-Vicuna (our custom
        tool included here).  
        Unlike Picard MarkDuplicates, M-Vicuna does not require reads to be
        previously aligned.
        Note that this requires that we see all reads for a given library
        at once, so we have to re-merge all of the split files, and then
        split them out again when we're done.
    '''
    input:  config["tmpDir"]+ '/'+config["subdirs"]["depletion"]+'/{sample}.bmtagger_depleted.bam'
    output: config["tmpDir"]+ '/'+config["subdirs"]["depletion"]+'/{sample}.rmdup.bam'
    resources: mem=3
    params: LSF='-W 4:00',
            logid="{sample}"
    shell:  "{config[binDir]}/read_utils.py rmdup_mvicuna_bam {input} {output}"

rule deplete_blastn:
    ''' This step depletes human reads using BLASTN, which is more sensitive,
        but much slower than BMTagger, so we run it last.
        This sometimes takes just over 4 hrs for highly dense lanes.
    '''
    input:  config["tmpDir"] +'/'+config["subdirs"]["depletion"]+'/{sample}.rmdup.bam'
    output: config["dataDir"]+'/'+config["subdirs"]["depletion"]+'/{sample}.cleaned.bam'
    resources: mem=10
    params: LSF='-q forest',
            refDbs = expand("{dbdir}/{db}", dbdir=config["blastDbDir"], db=config["blastDb_remove"]),
            logid="{sample}"
    shell:  "{config[binDir]}/taxon_filter.py deplete_blastn_bam {input} {params.refDbs} {output}"

rule filter_to_taxon:
    ''' This step reduces the read set to a specific taxon (usually the genus
        level or greater for the virus of interest).
    '''
    input:  config["dataDir"]+'/'+config["subdirs"]["depletion"]+'/{sample}.cleaned.bam'
    output: config["dataDir"]+'/'+config["subdirs"]["depletion"]+'/{sample}.taxfilt.bam'
    resources: mem=3
    params: LSF='-W 4:00',
            refDb=config["lastal_refDb"],
            logid="{sample}"
    shell:  "{config[binDir]}/taxon_filter.py filter_lastal_bam {input} {params.refDb} {output}"
"""


rule depletion:
    ''' This runs a full human read depletion pipeline, does PCR duplicate removal,
        and filters reads to a taxon of interest.
    '''
    input:  config["dataDir"]+'/'+config["subdirs"]["source"]+'/{sample}.bam'
    output: config["dataDir"]+'/'+config["subdirs"]["depletion"]+'/{sample}.raw.bam',
            config["tmpDir"]+ '/'+config["subdirs"]["depletion"]+'/{sample}.bmtagger_depleted.bam',
            config["tmpDir"] +'/'+config["subdirs"]["depletion"]+'/{sample}.rmdup.bam',
            config["dataDir"]+'/'+config["subdirs"]["depletion"]+'/{sample}.cleaned.bam',
            config["dataDir"]+'/'+config["subdirs"]["depletion"]+'/{sample}.taxfilt.bam'
    resources: mem=10
    params: LSF='-q forest',
            bmtaggerDbs = expand("{dbdir}/{db}", dbdir=config["bmTaggerDbDir"], db=config["bmTaggerDbs_remove"]),
            blastDbs = expand("{dbdir}/{db}", dbdir=config["blastDbDir"], db=config["blastDb_remove"]),
            lastDb=config["lastal_refDb"],
            logid="{sample}"
    run:
            makedirs(expand("{dir}/{subdir}",
                dir=[config["dataDir"],config["tmpDir"]],
                subdir=config["subdirs"]["depletion"]))
            shell("{config[binDir]}/taxon_filter.py deplete_human {input} {output[0]} {output[1]} {output[2]} {output[3]} --taxfiltBam {output[4]} --bmtaggerDbs {params.bmtaggerDbs} --blastDbs {params.blastDbs} --lastDb {params.lastDb}")


def merge_one_per_sample_inputs(wildcards):
    if 'seqruns_demux' not in config or not os.path.isfile(config['seqruns_demux']):
        return config["dataDir"]+'/'+config["subdirs"]["depletion"] +'/'+ wildcards.sample + '.' + wildcards.adjective + '.bam'
    runs = set()
    for flowcell in read_samples_file(config['seqruns_demux']):
        for lane in read_tab_file(flowcell):
            for well in read_tab_file(lane['barcode_file']):
                if well['sample'] == wildcards.sample:
                    runs.add(os.path.join(config["dataDir"], config["subdirs"]["depletion"],
                        get_run_id(well) +'.'+ lane['flowcell'] +'.'+ lane['lane'] +'.'+ wildcards.adjective + '.bam'))
    return sorted(runs)
rule merge_one_per_sample:
    ''' All of the above depletion steps are run once per flowcell per lane per
        multiplexed sample.  This reduces recomputation on the same data when
        additional sequencing runs are performed on the same samples.  This
        step merges reads down to one-per-sample, which all downstream
        analysis steps require.  For cleaned and taxfilt outputs, we re-run
        the rmdup step on a per-library basis after merging.
    '''
    input:  merge_one_per_sample_inputs
    output: config["dataDir"]+'/'+config["subdirs"]["per_sample"] +'/{sample}.{adjective,raw|cleaned|taxfilt}.bam'
    resources: mem=4
    params: LSF='-W 4:00',
            logid="{sample}-{adjective}",
            tmpf_bam=config["tmpDir"]+'/'+config["subdirs"]["depletion"] +'/{sample}.{adjective}.bam'
    run:
            makedirs(config["dataDir"]+'/'+config["subdirs"]["per_sample"])
            if wildcards.adjective == 'raw':
                shell("{config[binDir]}/read_utils.py merge_bams {input} {output} --picardOptions SORT_ORDER=queryname")
            else:
                shell("{config[binDir]}/read_utils.py merge_bams {input} {params.tmpf_bam} --picardOptions SORT_ORDER=queryname")
                shell("{config[binDir]}/read_utils.py rmdup_mvicuna_bam {params.tmpf_bam} {output}")

